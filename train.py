# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11XSQKOXYLw_5Ve8Rrr3tGeTZJo7r9QrY
"""

import wandb
from keras.datasets import fashion_mnist
from keras.datasets import mnist
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import math
import argparse

#wandb login

# For seeing examples of images
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)

wandb.init(project="Assignment 1")
run_name = "images_generating"
# Set the run name
wandb.run.name = run_name
wandb.run.save()

# this array keeps track for ind of each class
images_ind = []
# classes which are present
class_names = ["T-shirt/Top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle Boot"]

for i in range(10):
  for ind in range(X_train.shape[0]):
    if Y_train[ind] == i:
      images_ind.append(ind)
      break

images = []

for i in range(10):
  ind = images_ind[i]
  img = wandb.Image(X_train[ind], caption=[class_names[i]])
  images.append(img)

wandb.log({"Question 1": images})
wandb.finish()

plt.tight_layout()
plt.show()  # Display the plot



import numpy as np
import wandb
from tensorflow.keras.datasets import fashion_mnist, mnist

"""Activation Functions"""

def sigmoid(x):
    x = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x))

def sigmoid_grad(x):
    s = sigmoid(x)
    return s * (1 - s)

def tanh(x):
    return np.tanh(x)

def tanh_grad(x):
    return 1 - np.tanh(x)**2

def relu(x):
    return np.maximum(0, x)

def relu_grad(x):
    return (x > 0).astype(float)

def softmax(x):
    x = np.clip(x, -500, 500)
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

"""Weight Initialization Functions"""

def xavier_init(size):
    """ Xavier Initialization """
    fan_in, fan_out = size
    stddev = np.sqrt(2 / (fan_in + fan_out))  # sqrt(2 / (fan_in + fan_out))
    return np.random.randn(*size) * stddev

def random_init(size):
    """ Random initialization """
    return np.random.randn(*size)

"""Loss functions"""

def cross_entropy_loss(y_true, y_pred):
    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-9), axis=1))

def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

"""Optimizers"""

class Optimizer:
    def __init__(self, optimizer='adam', learning_rate=0.01, beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.0):
        self.optimizer = optimizer
        self.learning_rate = learning_rate
        self.beta = beta          # for momentum, nesterov, rmsprop
        self.beta1 = beta1        # for adam, nadam
        self.beta2 = beta2        # for adam, nadam
        self.epsilon = epsilon
        self.weight_decay = weight_decay
        self.m = {}               # momentum or first moment estimates
        self.v = {}               # second moment estimates (for adam/nadam, rmsprop)
        self.t = 0                # global timestep

    def update(self, keys, params, grads):
        """
        Update parameters given a list of unique keys, parameters, and their corresponding gradients.
        """
        self.t += 1  # update iteration count for bias correction

        if self.optimizer == "sgd":
            for key, param, grad in zip(keys, params, grads):
                if self.weight_decay > 0:
                    grad += self.weight_decay * param  # L2 regularization
                param -= self.learning_rate * grad

        elif self.optimizer == "momentum":
            for key, param, grad in zip(keys, params, grads):
                if key not in self.m:
                    self.m[key] = np.zeros_like(grad)
                if self.weight_decay > 0:
                    grad += self.weight_decay * param
                self.m[key] = self.beta * self.m[key] + (1 - self.beta) * grad
                param -= self.learning_rate * self.m[key]

        elif self.optimizer == "nesterov":
            for key, param, grad in zip(keys, params, grads):
                if key not in self.m:
                    self.m[key] = np.zeros_like(grad)
                prev_m = self.m[key].copy()
                if self.weight_decay > 0:
                    grad += self.weight_decay * param
                self.m[key] = self.beta * self.m[key] + (1 - self.beta) * grad
                # Nesterov update: lookahead using prev_m
                param -= self.learning_rate * (self.beta * prev_m + (1 - self.beta) * grad)

        elif self.optimizer == "rmsprop":
            for key, param, grad in zip(keys, params, grads):
                if key not in self.v:
                    self.v[key] = np.zeros_like(grad)
                if self.weight_decay > 0:
                    grad += self.weight_decay * param
                self.v[key] = self.beta * self.v[key] + (1 - self.beta) * (grad ** 2)
                param -= self.learning_rate * grad / (np.sqrt(self.v[key]) + self.epsilon)

        elif self.optimizer == "adam":
            for key, param, grad in zip(keys, params, grads):
                if key not in self.m:
                    self.m[key] = np.zeros_like(grad)
                    self.v[key] = np.zeros_like(grad)
                if self.weight_decay > 0:
                    grad += self.weight_decay * param
                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grad
                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grad ** 2)
                m_hat = self.m[key] / (1 - self.beta1 ** self.t)
                v_hat = self.v[key] / (1 - self.beta2 ** self.t)
                param -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)

        elif self.optimizer == "nadam":
            for key, param, grad in zip(keys, params, grads):
                if key not in self.m:
                    self.m[key] = np.zeros_like(grad)
                    self.v[key] = np.zeros_like(grad)
                if self.weight_decay > 0:
                    grad += self.weight_decay * param
                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grad
                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grad ** 2)
                m_hat = self.m[key] / (1 - self.beta1 ** self.t)
                v_hat = self.v[key] / (1 - self.beta2 ** self.t)
                # Nadam update: combines Nesterov momentum with Adam
                param -= self.learning_rate * ((self.beta1 * m_hat + (1 - self.beta1) * grad / (1 - self.beta1 ** self.t)) / (np.sqrt(v_hat) + self.epsilon))

"""Neural network class with forward and backward propogation"""

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, num_hidden_layers, output_size,
                 activation='relu', weight_init_type='Xavier', optimizer='adam',
                 learning_rate=0.01, batch_size=64, loss_type='cross_entropy',
                 beta=0.9, beta1=0.9, beta2=0.999, weight_decay=0.0005, iswandb=False,dataset="fashionmnist"):

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.output_size = output_size
        self.activation = activation
        self.loss_type = loss_type
        self.batch_size = batch_size
        self.iswandb = iswandb
        self.dataset = dataset

        self.optimizer = Optimizer(optimizer, learning_rate, beta, beta1, beta2, weight_decay)
        self.initialize_weights(weight_init_type)
        self.load_dataset(input_size)

    def initialize_weights(self, weight_init_type):
        """Initialize weights and biases."""
        self.weights, self.biases = [], []
        layer_sizes = [self.input_size] + [self.hidden_size] * self.num_hidden_layers + [self.output_size]
        for i in range(len(layer_sizes) - 1):
            if weight_init_type == 'Xavier':
                init_func = xavier_init
            else:
                init_func = random_init
            self.weights.append(init_func((layer_sizes[i], layer_sizes[i+1])))
            self.biases.append(np.zeros((1, layer_sizes[i+1])))

    def load_dataset(self, input_size):
        """Load and preprocess the dataset."""
        (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data() if self.dataset=="fashionmnist" else mnist.load_data()
        X_train, X_test = X_train / 255.0, X_test / 255.0
        X_train, X_test = X_train.reshape(-1, 784), X_test.reshape(-1, 784)
        y_train, y_test = np.eye(10)[y_train], np.eye(10)[y_test]

        split_idx = int(0.9 * len(X_train))
        self.X_train, self.y_train = X_train[:split_idx], y_train[:split_idx]
        self.X_val, self.y_val = X_train[split_idx:], y_train[split_idx:]
        self.X_test, self.y_test = X_test, y_test

    def activation_function(self, x):
        """Apply activation function based on the choice."""
        if self.activation == 'relu':
            return relu(x)
        elif self.activation == 'sigmoid':
            return sigmoid(x)
        elif self.activation == 'tanh':
            return tanh(x)
        return x  # Default: Linear activation

    def activation_gradient(self, x):
        """Compute gradient of activation function."""
        if self.activation == 'relu':
            return relu_grad(x)
        elif self.activation == 'sigmoid':
            return sigmoid_grad(x)
        elif self.activation == 'tanh':
            return tanh_grad(x)
        return np.ones_like(x)  # Default: Linear activation gradient

    def loss_function(self, y_true, y_pred):
        return cross_entropy_loss(y_true, y_pred) if self.loss_type == 'cross_entropy' else mse_loss(y_true, y_pred)

    def compute_accuracy(self, y_true, y_pred):
        return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))

    def forward(self, X):
        """Perform forward propagation."""
        self.activations, self.pre_activations = [X], []
        for i in range(len(self.weights) - 1):
            z = self.activations[-1] @ self.weights[i] + self.biases[i]
            self.pre_activations.append(z)
            self.activations.append(self.activation_function(z))
        z_out = self.activations[-1] @ self.weights[-1] + self.biases[-1]
        self.pre_activations.append(z_out)
        self.activations.append(softmax(z_out))
        return self.activations[-1]

    def backward(self, X, y_true):
        """Perform backward propagation with weight decay (L2 Regularization)."""
        grads = []
        loss_grad = self.activations[-1] - y_true
        grads.append(loss_grad)

        for i in range(len(self.weights) - 1, 0, -1):
            loss_grad = loss_grad @ self.weights[i].T * self.activation_gradient(self.pre_activations[i-1])
            grads.insert(0, loss_grad)

        weight_grads = [(self.activations[i].T @ grads[i]) + self.optimizer.weight_decay * self.weights[i] for i in range(len(self.weights))]
        bias_grads = [np.sum(grads[i], axis=0, keepdims=True) for i in range(len(self.weights))]

        # In the backward method, use separate keys for weights and biases:
        weight_keys = [f"w{i}" for i in range(len(self.weights))]
        bias_keys = [f"b{i}" for i in range(len(self.biases))]

        self.optimizer.update(weight_keys, self.weights, weight_grads)
        self.optimizer.update(bias_keys, self.biases, bias_grads)


    def fit(self, epochs):
        """Train the neural network with mini-batch gradient descent."""
        num_samples = self.X_train.shape[0]
        num_batches = num_samples // self.batch_size

        for epoch in range(epochs):
            indices = np.arange(num_samples)
            np.random.shuffle(indices)

            for batch in range(num_batches):
                batch_indices = indices[batch * self.batch_size : (batch + 1) * self.batch_size]
                X_batch, y_batch = self.X_train[batch_indices], self.y_train[batch_indices]

                y_pred_train = self.forward(X_batch)
                self.backward(X_batch, y_batch)

            train_loss = self.loss_function(self.y_train, self.forward(self.X_train))
            val_loss = self.loss_function(self.y_val, self.forward(self.X_val))
            train_acc = self.compute_accuracy(self.y_train, self.forward(self.X_train))
            val_acc = self.compute_accuracy(self.y_val, self.forward(self.X_val))

            if self.iswandb:
                wandb.log({
                    "epoch": epoch,
                    "train_loss": train_loss,
                    "val_loss": val_loss,
                    "train_accuracy": train_acc,
                    "val_accuracy": val_acc
                })

            print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")





# Create and run the sweep
#sweep_id = wandb.sweep(sweep=sweep_config, project='Assignment 1')
#wandb.agent(sweep_id, function=main, count=1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--wandb_entity", "-we", help="Wandb Entity used to track experiments in the Weights & Biases dashboard.", default="rajuguguloth7670-iitm-india")
    parser.add_argument("--wandb_project", "-wp", help="Project name used to track experiments in Weights & Biases dashboard", default="Assignment 1")
    parser.add_argument("--dataset", "-d", help="Dataset", choices=["mnist", "fashionmnist"], default="fashionmnist")
    parser.add_argument("--epochs", "-e", help="Number of epochs to train the neural network", type=int, default=10)
    parser.add_argument("--batch_size", "-b", help="Batch size used to train the neural network", type=int, default=64)
    parser.add_argument("--optimizer", "-o", help="Optimizer used to train the neural network", default="nadam", choices=["sgd", "momentum", "nag", "rmsprop", "adam", "nadam"])
    parser.add_argument("--loss", "-l", default="cross_entropy", choices=["mean_squared_error", "cross_entropy"])
    parser.add_argument("--learning_rate", "-lr", default=0.0001, type=float)
    parser.add_argument("--momentum", "-m", default=0.9, type=float)
    parser.add_argument("--beta", "-beta", default=0.9, type=float)
    parser.add_argument("--beta1", "-beta1", default=0.5, type=float)
    parser.add_argument("--beta2", "-beta2", default=0.5, type=float)
    parser.add_argument("--epsilon", "-eps", type=float, default=0.000001)
    parser.add_argument("--weight_decay", "-w_d", default=0.0005, type=float)
    parser.add_argument("-w", "--weight_init", default="Xavier", choices=["random", "Xavier"])
    parser.add_argument("--num_layers", "-nhl", type=int, default=4)
    parser.add_argument("--hidden_size", "-sz", type=int, default=64)
    parser.add_argument("-a", "--activation", choices=["identity", "sigmoid", "tanh", "relu"], default="tanh")

    args = parser.parse_args()
    

    
    args = parser.parse_args()
    # print(args.dataset)
    # print(args.epochs)
    # print(args.batch_size)
    # print(args.optimizer)
    # print(args.loss)
    # print(args.learning_rate)
    # print(args.momentum)
    # print(args.beta)
    # print(args.beta1)
    # print(args.beta2)
    # print(args.epsilon)
    # print(args.weight_decay)
    # print(args.weight_init)
    # print(args.num_layers)
    # print(args.hidden_size)
    # print(args.activation)

    wandb.login()
    wandb.init(project=args.wandb_project,entity=args.wandb_entity)
    model = NeuralNetwork(
        input_size=784,                    
        hidden_size=args.hidden_size,
        num_hidden_layers=args.num_layers,
        output_size=10,                    
        activation=args.activation,
        weight_init_type=args.weight_init,
        optimizer=args.optimizer,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        loss_type=args.loss,          
        weight_decay=args.weight_decay,
        iswandb=True,
        dataset=args.dataset)

    model.fit(epochs=args.epochs)
    wandb.finish()
